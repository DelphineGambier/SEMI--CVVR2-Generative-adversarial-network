{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Apple_Generator_Final_Version.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Xbj4JjP-mAivDpYMQ9ZocUJA16rCdTQM","authorship_tag":"ABX9TyO+njJ75+dVmXrVvOK+S/px"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cuHfgdI5y6bJ"},"source":["# Apple Generator\n","\n","**Warning :** *this GAN is not functional but is published to find ways to improve it*"]},{"cell_type":"markdown","metadata":{"id":"ATlkdW_SyzKg"},"source":["## Required installations and imports "]},{"cell_type":"code","metadata":{"id":"us1BqOP1UTmz"},"source":["!pip install tensorflow "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3RWsFAsuUiQG"},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","from tensorflow.keras import layers\n","import time\n","\n","from IPython import display"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CGVrvn8MYJjH"},"source":["## Creation of training data"]},{"cell_type":"code","metadata":{"id":"iXlaE-M6yr4R"},"source":["image_size = (256, 256)\n","batch_size = 32\n","directory = \"/Clean_Apple/\" # Images directory\n","\n","\n","train_apples = tf.keras.preprocessing.image_dataset_from_directory(\n","    directory,\n","    seed=123,\n","    color_mode='rgb', #imags rgb\n","    image_size=image_size,\n","    batch_size=batch_size,\n","    label_mode= None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VL3GySYemkuc"},"source":["### Image normalization\n"]},{"cell_type":"code","metadata":{"id":"evfd5Aqhmsa2"},"source":["# normalize the images to [-1, 1]\n","def normalize(image):\n","  image = tf.cast(image, tf.float32)\n","  image = (image / 127.5) - 1\n","  return image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHZHZzO1m35I"},"source":["BUFFER_SIZE = 1000\n","BATCH_SIZE = 1\n","\n","train_apples = train_apples .map(\n","    normalize, num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(\n","    BUFFER_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0U7YEBIpt7U"},"source":["## Model creation"]},{"cell_type":"markdown","metadata":{"id":"fWu9ubwipyJU"},"source":["### Generator"]},{"cell_type":"code","metadata":{"id":"hETRLS91p2A0"},"source":["def make_generator_model():\n","    model = tf.keras.Sequential()\n","    model.add(layers.Dense( 64*64*256 , use_bias=False, input_shape=(100,)))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Reshape((64,64,256)))\n","    assert model.output_shape == (None,64,64,256)  # Note: ce n'est pas le batch_size\n","\n","    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1,1), padding='same', use_bias=False))\n","    assert model.output_shape == (None,64,64,128)\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2,2), padding='same', use_bias=False))\n","    assert model.output_shape == (None, 128,128,64)\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n","    assert model.output_shape == (None, 256, 256, 3)\n","\n","\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZn5aBfgraty"},"source":["generator = make_generator_model()\n","\n","noise = tf.random.normal([1, 100])\n","generated_image = generator(noise, training=False)\n","\n","plt.imshow(generated_image[0, :, :, 0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EX1qXxvzTG0O"},"source":["### Discriminator"]},{"cell_type":"code","metadata":{"id":"YfTLZbayTSuI"},"source":["def make_discriminator_model():\n","    model = tf.keras.Sequential()\n","\n","    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n","                                     input_shape=[256, 256, 3]))\n","            \n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(1))\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3GDS83FTpZy"},"source":["discriminator = make_discriminator_model()\n","decision = discriminator(generated_image)\n","print (decision)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3nT9EYl0T4Au"},"source":["## Loss function and optimizer"]},{"cell_type":"code","metadata":{"id":"7_5XOxMq7FZd"},"source":["# This method returns a helper function to compute cross entropy loss\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mt9sOLOxT3HM"},"source":["def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CL3ezFfuT_Uo"},"source":["def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnft3x12UN-u"},"source":["generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjWCQf0lUYQd"},"source":["## Saving the checkpoints"]},{"cell_type":"code","metadata":{"id":"yR7arcDqUff_"},"source":["checkpoint_dir = '/Checkpoints' # checkpoints directory \n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUlJCekYUpzZ"},"source":["## Training\n"]},{"cell_type":"code","metadata":{"id":"Yp3g4jkyU1VX"},"source":["EPOCHS = 100\n","noise_dim = 100\n","num_examples_to_generate = 1\n","\n","seed = tf.random.normal([num_examples_to_generate, noise_dim])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6E9bEIUmkFmD"},"source":["noise = tf.random.normal([BATCH_SIZE, noise_dim])\n","\n","@tf.function\n","def train_step(images):\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","      generated_images = generator(noise, training=True)\n","\n","      real_output = discriminator(images, training=True)\n","      fake_output = discriminator(generated_images, training=True)\n","\n","      gen_loss = generator_loss(fake_output)\n","      disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VJYUNNu3kJ_o"},"source":["def train(dataset, epochs):\n","  for epoch in range(epochs):\n","    start = time.time()\n","\n","    for image_batch in dataset:\n","      train_step(image_batch)\n","\n","    # Produce images \n","    display.clear_output(wait=True)\n","    generate_and_save_images(generator, epoch + 1, seed)\n","\n","    # Save the model every 15 epochs\n","    if (epoch + 1) % 15 == 0:\n","      checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n","\n","  # Generate after the final epoch\n","  display.clear_output(wait=True)\n","  generate_and_save_images(generator, epochs,seed)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1AbUXqdkQag"},"source":["def generate_and_save_images(model, epoch, test_input):\n","  predictions = model(test_input, training=False)\n","\n","  fig = plt.figure()\n","\n","  for i in range(predictions.shape[0]):\n","      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.)\n","      plt.axis('off')\n","  plt.savefig('/Training_images/img{:04d}.png'.format(epoch)) # saving images during the training\n","  plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7hBj7Oikaji"},"source":["train(train_apples, EPOCHS)"],"execution_count":null,"outputs":[]}]}